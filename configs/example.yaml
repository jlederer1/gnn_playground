# copy or original configs/example.yaml

### This is a configuration file for the hyperparameters of our GNN competition. ###

# Dataset & tasks
data:
  # Note: Dataset and task must be compatible!                            (TODO: add checks or automation)
  name: MUTAG            # KarateClub | MUTAG | ENZYMES  
  task: graph            # graph | node 
  device: cpu            # cuda | mps | cpu | auto (TODO)
  random_seed: 42 

# Model architecture
model:
  num_layers: 3           # Number of GNN layers
  hidden_dim: 64          # Size of hidden layers embeddings
  dropout_value: 0.5      # Dropout rate for regularization 
  layer_type: GCN         # GCN | GAT | GraphSAGE | GIN
  aggregator: mean        # mean | sum | max (only for GraphSAGE)
  update_func: MLP        # MLP  | BLSTM (only for GIN)                   (TODO: add Attention)
  glob_pooler: sum        # mean | add | max (for graph tasks)
  activation: default     # relu | leaky_relu | sigmoid | tanh | default (according to original papers)

optimizer: 
  type: Adam              # Adam | SGD 
  lr: 0.001               # Learning rate 
  weight_decay: 0.0001    # Decay rate for L2 regularization

training:
  batch_size: 32
  epochs: 100
  val_split: 0.5 
