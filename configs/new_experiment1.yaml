# copy or original configs/example.yaml

### This is a configuration file for the hyperparameters of our GNN competition. ###

# Dataset & tasks
data:
  # Note: Dataset and task must be compatible!                            (TODO: add checks or automation)
  name: KarateClub       # KarateClub | MUTAG   
  task: node            # graph | node 
  device: cpu            # cuda | mps | cpu | auto (TODO)
  random_seed: 42 

# Model architecture
model:
  num_layers: 2           # Number of GNN layers
  hidden_dim: 128         # Size of hidden layers embeddings
  dropout_value: 0.5      # Dropout rate for regularization 
  layer_type: GIN         # GCN | GAT | GraphSAGE | GIN
  aggregator: mean        # mean | sum | max (only for GraphSAGE)
  update_func: BLSTM      # MLP  | BLSTM (only for GIN)                   (TODO: add Attention)
  glob_pooler: sum        # mean | add | max (only for graph tasks)
  activation: leaky_relu  # relu | leaky_relu | sigmoid | tanh | default (according to original papers)

optimizer: 
  type: Adam              # Adam | SGD 
  lr: 0.001               # Learning rate 
  weight_decay: 0.0001    # Decay rate for L2 regularization

training:
  batch_size: 32
  epochs: 100
  val_split: 0.5 
